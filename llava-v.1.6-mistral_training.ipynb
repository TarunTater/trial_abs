{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users1/knupleun/home/projects/TARUN_TEST/venv/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/users1/knupleun/home/projects/TARUN_TEST/venv/lib64/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "No ROCm runtime is found, using ROCM_HOME='/usr'\n"
     ]
    }
   ],
   "source": [
    "# dpo_idefics2-8b.py\n",
    "from datasets import features, load_dataset\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "import torch\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and processor\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=\"./cache\",\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"llava-hf/llava-v1.6-mistral-7b-hf\", do_image_splitting=False, cache_dir=\"./cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\n",
    "    \"openbmb/RLAIF-V-Dataset\", split=\"train[:500]\", cache_dir=\"cache\"\n",
    ")\n",
    "\n",
    "\n",
    "def format(example):\n",
    "    # Prepare the input for the chat template\n",
    "    prompt = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": example[\"question\"]},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    chosen = [\n",
    "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": example[\"chosen\"]}]}\n",
    "    ]\n",
    "    rejected = [\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": example[\"rejected\"]}],\n",
    "        }\n",
    "    ]\n",
    "    # Apply the chat template\n",
    "    prompt = processor.apply_chat_template(prompt, tokenize=False)\n",
    "    chosen = processor.apply_chat_template(chosen, tokenize=False)\n",
    "    rejected = processor.apply_chat_template(rejected, tokenize=False)\n",
    "    # Resize the image to ensure it fits within the maximum allowable\n",
    "    # size of the processor to prevent OOM errors.\n",
    "    # max_size = processor.image_processor.size[\"longest_edge\"] // 2\n",
    "    # example[\"image\"].thumbnail((max_size, max_size))\n",
    "    return {\n",
    "        \"images\": [example[\"image\"]],\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "dataset = dataset.map(format, remove_columns=dataset.column_names, num_proc=32)\n",
    "\n",
    "# Make sure that the images are decoded, it prevents from storing bytes.\n",
    "# More info here https://github.com/huggingface/blog/pull/2148#discussion_r1667400478\n",
    "f = dataset.features\n",
    "f[\"images\"] = features.Sequence(features.Image(decode=True))\n",
    "dataset = dataset.cast(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"idefics2-8b-dpo\",\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    dataset_num_proc=8,  # tokenization will use 32 processes\n",
    "    dataloader_num_workers=32,  # data loading will use 32 workers\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting prompt from train dataset (num_proc=8): 100%|██████████| 500/500 [00:31<00:00, 15.95 examples/s]\n",
      "Applying chat template to train dataset (num_proc=8): 100%|██████████| 500/500 [00:00<00:00, 698.58 examples/s] \n",
      "Tokenizing train dataset (num_proc=8): 100%|██████████| 500/500 [00:08<00:00, 61.89 examples/s] \n"
     ]
    }
   ],
   "source": [
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None,  # not needed when using peft\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=processor,\n",
    "    peft_config=LoraConfig(target_modules=\"all-linear\", r=32),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/users1/knupleun/home/projects/TARUN_TEST/venv/lib64/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4/62 04:45 < 2:18:04, 0.01 it/s, Epoch 0.05/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
